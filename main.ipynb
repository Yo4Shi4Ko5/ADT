{"cells":[{"cell_type":"markdown","metadata":{"id":"p71G9lgOjTOV"},"source":["# google driveのマウント"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouL4S2AJo-7f"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"x-KHoHd9MFCj"},"source":["# 準備"]},{"cell_type":"markdown","metadata":{"id":"sQwOb86FjiJJ"},"source":["## ライブラリのインポート"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4980,"status":"ok","timestamp":1642218886173,"user":{"displayName":"大石芳弘","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16410462591550827321"},"user_tz":-540},"id":"222_xNCaR9x2","outputId":"738ace20-5716-4d25-c4ba-7763531a5218"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mir_eval\n","  Downloading mir_eval-0.6.tar.gz (87 kB)\n","\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 51 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 81 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.19.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir_eval) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.15.0)\n","Building wheels for collected packages: mir-eval\n","  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mir-eval: filename=mir_eval-0.6-py3-none-any.whl size=96515 sha256=b26730170a8a87708133d3afdfdf5f54ae861a0002141126489e34601fa2735b\n","  Stored in directory: /root/.cache/pip/wheels/08/28/2d/006dbad29550bac8daf049ff34fa882655a7d3e77f3b67595e\n","Successfully built mir-eval\n","Installing collected packages: mir-eval\n","Successfully installed mir-eval-0.6\n"]}],"source":["!pip install mir_eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZ1xav7opCdg"},"outputs":[],"source":["import os\n","import numpy as np\n","import librosa\n","import math\n","import mir_eval\n","from tqdm import tqdm\n","import xml.dom.minidom\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import pickle\n","import matplotlib.pyplot as plt\n","import warnings\n","import sys\n","from collections import defaultdict\n","import glob"]},{"cell_type":"markdown","metadata":{"id":"zJoKYrjgkBqI"},"source":["## グローバル変数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tA36Z-UHpMPG"},"outputs":[],"source":["EPS = 10e-6\n","\n","SR = 8000\n","SR_WAV = SR\n","DURATION = 4\n","NSP_SRC = SR * DURATION\n","N_FFT = 1024\n","HOP = 1024 // 2\n","\n","\n","EVALDATA_PATH = \"/content/drive/My Drive/data_evals\"\n","DATA_PATH = \"/content/drive/My Drive/data_drum_sources\"\n","STEM_PATH1 = \"/content/drive/My Drive/stems_synth\"\n","STEM_PATH2 = \"/content/drive/My Drive/stems_GMD\"\n","MODEL_PATH = \"/content/drive/My Drive/saved_models\"\n","RESULT_PATH = \"/content/drive/My Drive/results\"\n","\n","USE_CUDA = torch.cuda.is_available()\n","\n","torch.set_default_tensor_type('torch.FloatTensor')\n","\n","if USE_CUDA:\n","  DEVICE = torch.device('cuda:0')\n","else:\n","  DEVICE = 'cpu'\n","\n","\n","NPDTYPE = np.float32\n","TCDTYPE = torch.get_default_dtype()\n","\n","DRUM_NAMES = [\"KD_KD\", \"SD_SD\", \"HH_CHH\", \"HH_OHH\", \"HH_PHH\", \"TT_HIT\",\n","              \"TT_MHT\", \"TT_HFT\", \"CY_RDC\", \"CY_RDB\", \"CY_CRC\", \"CY_CHC\",\n","              \"CY_SPC\", \"OT_TMB\", \"OT_CL\", \"OT_CB\"]\n","N_DRUM_VSTS = 12"]},{"cell_type":"markdown","metadata":{"id":"UTXTBVj3bK0a"},"source":["## 変数(モデル)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsWn8OqabQu9"},"outputs":[],"source":["# モデル\n","kernel_size = 3  # 奇数\n","n_channel = 50\n","scale_r = 2\n","activation = \"elu\"\n","enc_layers = 10\n","dec_layers = 7\n","\n","tran_layers = 3\n","tran_heads = 5\n","d_ffn = 200\n","dropout = 0.1\n","\n","sparsemax_lst = 32\n","\n","# 訓練\n","batch_size = 4\n","loss_domains = [\"spectrum\"]  # melgram, stft, l1_reg\n","n_mels = 0\n","learning_rate = 0.00025\n","metrics = [\"mae\"]  # mse\n","source_norm = \"sqrsum\"  # no, abssum, sqrsum\n","l1_reg_lambda = 0.003\n","cqt_bins = 12\n","compare_after_hpss = False"]},{"cell_type":"markdown","metadata":{"id":"zcCsnkioD1qG"},"source":["## ドラム音のロード"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeWRh3HpEeRB"},"outputs":[],"source":["# テストドラム音(1つずつ)のロード\n","def load_drum_srcs(idx=1):\n","  dura = 1.0\n","  srcs = []\n","  for drum_note_name in DRUM_NAMES:\n","    filename = \"%d)%s.wav\" % (idx, drum_note_name)\n","    src, _ = librosa.load(os.path.join(DATA_PATH, filename), sr=SR_WAV, duration=dura)\n","    srcs.append(src)\n","  inst_srcs = torch.from_numpy(np.array(srcs))\n","  return inst_srcs\n","\n","\n","# 学習時使用ドラム音のロード\n","\n","def get_drumsets(norm):\n","  if norm == \"abssum\":\n","    norm_func = lambda x: 400 * 1.0 / (x.abs().sum())\n","  elif norm == \"sqrsum\":\n","    norm_func = lambda x: 10 * 1.0 / x.pow(2).sum().sqrt()\n","  else:\n","    norm_func = lambda x: 1\n","\n","  notes = {key: [] for key in DRUM_NAMES}\n","\n","  for i in range(1, N_DRUM_VSTS):\n","    srcs = load_drum_srcs(i)\n","\n","    for src, key in zip(srcs, DRUM_NAMES):\n","      notes[key].append(norm_func(src) * src)\n","\n","  for note in notes:\n","    srcs = notes[note]\n","    for i, src in enumerate(srcs):\n","      srcs[i] = torch.flip(src, dims=(0,))\n","    notes[note] = tuple(notes[note])\n","\n","  return DrumSourceSet(notes=notes)\n","\n","\n","class DrumSourceSet(object):\n","  def __init__(self, notes, reverse=True):\n","    self.n_notes = len(notes)\n","    self.notes = notes.copy()\n","    self.note_names = list(self.notes.keys())\n","    self.reverse = reverse\n","    self.n_vsts = {note: len(self.notes[note]) for note in self.note_names}\n","\n","  def __getitem__(self, note):\n","    return self.notes[note]\n","\n","  def random_choice(self, note):\n","    return self.notes[note][np.random.choice(self.n_vsts[note])]\n","\n","  def __str__(self):\n","    return (\"n_notes: %d, n_vsts:\" % self.n_notes) + str(self.n_vsts)"]},{"cell_type":"markdown","metadata":{"id":"toU9W9I04ghm"},"source":["## モデル"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8nBsxpu4zSh"},"outputs":[],"source":["class DrumTranModel(nn.Module):\n","  def __init__(self, inst_srcs, inst_names, drum_sets):\n","    super().__init__()\n","    self.test_inst_srcs = inst_srcs\n","    self.test_inst_names = inst_names\n","    self.drum_sets = drum_sets\n","    self.n_notes = self.drum_sets.n_notes\n","\n","    self.unet = Unet(nn.Conv1d, nn.MaxPool1d)\n","    Rec = TransformerEncoder  # Recurrenter, Convoluter\n","    self.recurrenter = Rec(tran_layers, n_channel, tran_heads, d_ffn, dropout, self.n_notes)  # Rec(n_ch_repre, hidden_size=self.n_notes) n_ch_repre=n_channel*2\n","    SpMax = MultiplySparsemax  # SequentialSparsemax, SoftSoftSeq, SoftSoftMul\n","    self.sparsemax_lst = sparsemax_lst\n","    self.double_sparsemax = SpMax(self.sparsemax_lst)\n","    self.zero_inserter = ZeroInserter(self.unet.sr_ratio)\n","    self.synthesizer = FastDrumSynthesizer(self.n_notes, self.drum_sets)\n","    self.mixer = Mixer()\n","\n","    \"\"\"\n","    print('NUM_PARAM overall:', count_parameters(self))\n","    print('             unet:', count_parameters(self.unet))\n","    print('      recurrenter:', count_parameters(self.recurrenter))\n","    print('       sparsemaxs:', count_parameters(self.double_sparsemax))\n","    print('      synthesizer:', count_parameters(self.synthesizer))\n","    \"\"\"\n","\n","  def forward(self, x):\n","    nsp_src = x.shape[1]\n","    div = self.unet.compress_ratio\n","    nsp_pad = (div - (nsp_src % div)) % div\n","    if nsp_pad != 0:\n","      x = F.pad(x, (0, nsp_pad))\n","\n","    r = self.unet(x)\n","    dense_y = self.recurrenter(r)\n","    sparse_y = self.double_sparsemax(dense_y)\n","    y_hat = upsampled_y = self.zero_inserter(sparse_y)\n","    tracks = self.synthesizer(y_hat)\n","    x_hat = est_mix = self.mixer(tracks)\n","\n","    # trimmed = (x.shape[1] - x_hat.shape[1]) // 2\n","    # x_trimmed = x[:, trimmed: -trimmed]\n","    return x, x_hat, y_hat"]},{"cell_type":"markdown","metadata":{"id":"4MKGHSy4AIb9"},"source":["## モデルモジュール - Unet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fiXbj22AGIa"},"outputs":[],"source":["class Unet(nn.Module):\n","  def __init__(self, conv, mp):\n","    super().__init__()\n","    self.n_channel = n_channel\n","    self.kernel_size = kernel_size\n","    self.padding = self.kernel_size // 2\n","    self.act = F.elu  # F.relu, F.leaky_relu\n","    self.scale_r = scale_r\n","    self.enc_layers = enc_layers\n","    self.dec_layers = dec_layers\n","    self.sr_ratio = self.scale_r ** (self.enc_layers - self.dec_layers)\n","    self.compress_ratio = self.scale_r ** self.enc_layers\n","\n","    n_ch, k_size, pd = self.n_channel, self.kernel_size, self.padding\n","    first_ch = min(128, n_ch)\n","    st = 1\n","    bias = False  # True\n","    # self.ch_out = 2 * n_ch\n","\n","    self.d_conv0 = conv(1, first_ch, k_size, st, pd, bias=bias)\n","\n","    self.d_convs = nn.ModuleList([conv(first_ch, n_ch, k_size, st, pd, bias=bias)] +\n","                                 [conv(n_ch, n_ch, k_size, st, pd, bias=bias) \n","                                 for _ in range(self.enc_layers - 1)])\n","    self.pools = nn.ModuleList([mp(self.scale_r) for _ in range(self.enc_layers)])\n","    # self.encode_conv = conv(n_ch, n_ch, k_size, st, pd, bias=bias)\n","    self.u_convs = nn.ModuleList([conv(n_ch, n_ch, k_size, st, pd, bias=bias)] +\n","                                 [conv(2 * n_ch, n_ch, k_size, st, pd, bias=bias)\n","                                 for _ in range(self.dec_layers - 1)])\n","    self.last_conv = conv(2 * n_ch, n_ch, self.kernel_size, st, pd)\n","\n","  def forward(self, x):\n","    x = torch.unsqueeze(x, 1)\n","    x = self.act(self.d_conv0(x))\n","    xs = []\n","    for pool, conv in zip(self.pools, self.d_convs):\n","      x = conv(x)\n","      x = pool(self.act(x))\n","      xs.append(x)\n","\n","    # ys = []\n","    y = xs.pop()\n","    # y_end = self.encode_conv(xs.pop())\n","    # y = self.act(y_end)\n","    # ys.append(y)\n","    for conv in self.u_convs:\n","      y = conv(y)\n","      y = self.act(y)\n","      y = F.interpolate(y, scale_factor=self.scale_r,\n","                        mode=int(y.dim()==4)*\"bi\"+\"linear\", align_corners=False)\n","      x = xs.pop()\n","      # crop = (x.shape[2] - y.shape[2]) // 2\n","      # x = x[:, :, crop:-crop]\n","      y = torch.cat((y, x), dim=1)\n","      # ys.append(y)\n","\n","    r = self.last_conv(y)\n","    return self.act(r)"]},{"cell_type":"markdown","metadata":{"id":"riOJUf-ktbs0"},"source":["## モデルモジュール - Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arYs5SdCtaDS"},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","  def __init__(self, n_layers, d_model, n_heads, d_feedfoward, dropout, n_insts):\n","    super().__init__()\n","    self.pos_encoder = PositionalEncoding(d_model)\n","    self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_feedfoward, dropout)\n","                                 for _ in range(n_layers)])\n","    self.linear = nn.Linear(d_model, n_insts)\n","\n","  def forward(self, src):\n","    src = src.transpose(1, 2)\n","    src = self.pos_encoder(src)\n","    for layer in self.layers:\n","      src = layer(src)\n","    src = self.linear(src)\n","    src = src.transpose(1, 2)\n","    return src\n","\n","\n","class TransformerEncoderLayer(nn.Module):\n","  def __init__(self, d_model, n_heads, d_feedforward, dropout):\n","    super().__init__()\n","    d_k = d_model // n_heads\n","    self.mul_h_attention = MultiHeadAttention(n_heads, d_model, d_k)\n","    self.norm = nn.LayerNorm(d_model)\n","    self.feedforward = nn.Sequential(nn.Linear(d_model, d_feedforward), \n","                                     nn.ReLU(),\n","                                     nn.Linear(d_feedforward, d_model))\n","\n","  def forward(self, src):\n","    src = src + self.mul_h_attention(src)\n","    src = self.norm(src)\n","    src = self.feedforward(src)\n","    return src\n","\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, n_heads, d_model, d_k):\n","    super().__init__()\n","    self.heads = nn.ModuleList([AttentionHead(d_model, d_k) for _ in range(n_heads)])\n","  \n","  def forward(self, src):\n","    src = torch.cat([head(src) for head in self.heads], dim=-1)\n","    return src\n","\n","\n","class AttentionHead(nn.Module):\n","  def __init__(self, d_model, d_k):\n","    super().__init__()\n","    self.q = nn.Linear(d_model, d_k)\n","    self.k = nn.Linear(d_model, d_k)\n","    self.v = nn.Linear(d_model, d_k)\n","    self.softmax = nn.Softmax(dim=-1)\n","  \n","  def forward(self, src):\n","    q, k, v = self.q(src), self.k(src), self.v(src)\n","    temp = q.bmm(k.transpose(1, 2))\n","    scale = q.size(-1) ** 0.5\n","    softmax = self.softmax(temp / scale)\n","    return softmax.bmm(v)\n","\n","\n","class PositionalEncoding(nn.Module):\n","  def __init__(self, d_model, max_len=100000, dropout=0.1):\n","    super().__init__()\n","    position = torch.arange(max_len).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n","    pe = torch.zeros(1, max_len, d_model)\n","    pe[0, :, 0::2] = torch.sin(position * div_term)\n","    pe[0, :, 1::2] = torch.cos(position * div_term)\n","    self.register_buffer(\"pe\", pe)\n","    self.dropout = nn.Dropout(p=dropout)\n","  \n","  def forward(self, x):\n","    x = x + self.pe[:, :x.size(1), :]\n","    x = self.dropout(x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"dZyDzppVuDxr"},"source":["## モデルモジュール - sparsemax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otN_EjGOuabb"},"outputs":[],"source":["class MultiplySparsemax(nn.Module):\n","  def __init__(self, sparsemax_lst=64):\n","    super().__init__()\n","    self.lst = sparsemax_lst\n","    self.sparsemax_inst = Sparsemax(dim=-1)\n","    self.sparsemax_time = Sparsemax(dim=-1)\n","    self.softmax_time = nn.Softmax(dim=-1)\n","\n","  def forward(self, midis_out):\n","    batch, n_insts, time = midis_out.shape\n","    lst = self.lst\n","    len_pad = (lst - time % lst) % lst\n","\n","    midis_out = F.pad(midis_out, [0, len_pad])\n","    midis_out_inst = self.sparsemax_inst(midis_out.transpose(1, 2)).transpose(1, 2)\n","\n","    midis_out_hh = midis_out[:, 2:5, :]\n","    midis_out_hh_time = midis_out_hh.reshape(batch, 3, (time + len_pad) // lst, lst)\n","    midis_out_hh_time = self.softmax_time(midis_out_hh_time)\n","    midis_out_hh_time = midis_out_hh_time.reshape(batch, 3, (time + len_pad))\n","\n","    midis_out_others = torch.cat((midis_out[:, :2, :], midis_out[:, 5:, :]), 1)\n","    midis_out_others_time = midis_out_others.reshape(batch, n_insts-3, (time + len_pad) // lst, lst)\n","    midis_out_others_time = self.sparsemax_time(midis_out_others_time)\n","    midis_out_others_time = midis_out_others_time.reshape(batch, n_insts-3, (time + len_pad))\n","\n","    midis_out_time = torch.cat((midis_out_others_time[:, :2, :], midis_out_hh_time, midis_out_others_time[:, 2:, :]), 1)\n","\n","    # midis_out_time = midis_out.reshape(batch, n_insts, (time + len_pad) // lst, lst)\n","    # midis_out_time = self.sparsemax_time(midis_out_time)\n","    # midis_out_time = midis_out_time.reshape(batch, n_insts, (time + len_pad))\n","    \n","    midis_final = midis_out_inst[:, :, :time] * midis_out_time[:, :, :time]\n","    return midis_final\n","\n","\n","class Sparsemax(nn.Module):\n","  def __init__(self, dim=-1):\n","    super().__init__()\n","    self.dim = dim\n","\n","  def forward(self, input):\n","    original_size = input.size()\n","    input = input.contiguous().view(-1, input.size(self.dim))\n","\n","    dim = 1\n","    number_of_logits = input.size(dim)  # 11\n","\n","    input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n","\n","    zs = torch.sort(input=input, dim=dim, descending=True)[0]\n","    range = torch.arange(start=1, end=number_of_logits + 1, device=input.device).view(1, -1)\n","    range = range.expand_as(zs).type(TCDTYPE)\n","\n","    bound = 1 + range * zs\n","    cumulative_sum_zs = torch.cumsum(zs, dim)\n","    is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n","    k = torch.max(is_gt * range, dim, keepdim=True)[0]\n","\n","    zs_sparse = is_gt * zs\n","\n","    taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n","    taus = taus.expand_as(input)\n","\n","    self.output = torch.max(torch.zeros_like(input), input - taus)\n","\n","    output = self.output.view(original_size)\n","    return output\n","\n","  def backward(self, grad_output):\n","    dim = 1\n","\n","    nonzeros = torch.ne(self.output, 0)\n","    sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n","    self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n","\n","    return self.grad_input"]},{"cell_type":"markdown","metadata":{"id":"zmRjpEQtuLuw"},"source":["## モデルモジュール - 0インサーター"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoF7ls1zwniR"},"outputs":[],"source":["class ZeroInserter(nn.Module):\n","  def __init__(self, insertion_rate):\n","    super().__init__()\n","    self.insertion_rate = insertion_rate\n","\n","  def forward(self, downsampled_y):\n","    batch, ch, time = downsampled_y.shape\n","    upsampled_y = []\n","    for ch_idx in range(ch):\n","      ds_y = downsampled_y[:, ch_idx:ch_idx + 1, :]\n","      us_y = torch.cat((ds_y,\n","                        torch.zeros((batch, self.insertion_rate - 1, time),\n","                                    device=downsampled_y.device)), dim=1)\n","      us_y = us_y.transpose(2, 1)\n","      us_y = torch.reshape(us_y, (batch, 1, self.insertion_rate * time))\n","      upsampled_y.append(us_y)\n","\n","    upsampled_y = torch.cat(upsampled_y, dim=1)\n","    return upsampled_y"]},{"cell_type":"markdown","metadata":{"id":"3OnThtr-uR5J"},"source":["## モデルモジュール - ドラムシンセサイザー"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Egb_4GPIxonZ"},"outputs":[],"source":["class FastDrumSynthesizer(nn.Module):\n","  def __init__(self, n_notes, drum_sets):\n","    super().__init__()\n","    self.drum_sets = drum_sets\n","    self.n_notes = n_notes\n","\n","  def forward(self, midis):\n","    device_ = midis[0].device\n","\n","    rv_insts = [self.drum_sets.random_choice(note_name).to(device_) for note_name in DRUM_NAMES]\n","\n","    tracks = []\n","    for i in range(self.n_notes):\n","      md = midis[:, i:i + 1, :]\n","      track = fast_conv1d(md, torch.flip(rv_insts[i].expand(1, 1, -1), dims=(2,)))\n","      tracks.append(track)\n","\n","    return torch.cat(tracks, dim=1)\n","\n","\n","# def complex_mul(t1, t2):\n","#   if t1.dim() != t2.dim():\n","#     raise ValueError('dim mismatch in complex_mul, {} and {}'.format(t1.dim(), t2.dim()))\n","\n","#   if t1.dim() == 2:\n","#     r1, i1 = t1[:, 0], t1[:, 1]\n","#     r2, i2 = t2[:, 0], t2[:, 1]\n","#   elif t1.dim() == 3:\n","#     r1, i1 = t1[:, :, 0], t1[:, :, 1]\n","#     r2, i2 = t2[:, :, 0], t2[:, :, 1]\n","#   elif t1.dim() == 4:\n","#     r1, i1 = t1[:, :, :, 0], t1[:, :, :, 1]\n","#     r2, i2 = t2[:, :, :, 0], t2[:, :, :, 1]\n","#   else:\n","#     raise NotImplementedError\n","  \n","#   return torch.stack([r1*r2-i1*i2, r1*i2+i1*r2], dim=-1)\n","\n","\n","# def _rfft(x, signal_ndim=1, normalized=False, onesided=True):\n","#   odd_shape1 = (x.shape[1] % 2 != 0)\n","#   x_shape = x.shape\n","#   x = torch.fft.rfft(x)\n","#   x = torch.cat([x.real.unsqueeze(dim=2), x.imag.unsqueeze(dim=2)], dim=2)\n","#   if onesided == False:\n","#     _x = x[:, 1:, :].flip(dims=[1]).clone() if odd_shape1 else x[:, 1:-1, :].flip(dims=[1]).clone()\n","#     _x[:,:,1] = -1 * _x[:,:,1]\n","#     x = torch.cat([x, _x], dim=1)\n","#   if normalized == True:\n","#     p = 1\n","#     for i in x_shape:\n","#       p *= i\n","#     x /= math.sqrt(p)\n","#   return x\n","\n","\n","# def _irfft(x, signal_sizes, signal_ndim=1, normalized=False, onesided=True):\n","#   x_shape = x.shape\n","#   if onesided == False:\n","#     res_shape1 = x.shape[1]\n","#     x = x[:,:(x.shape[1] // 2 + 1),:]\n","#     x = torch.complex(x[:,:,0].float(), x[:,:,1].float())\n","#     x = torch.fft.irfft(x, n=res_shape1)\n","#   else:\n","#     x = torch.complex(x[:,:,0].float(), x[:,:,1].float())\n","#     x = torch.fft.irfft(x)\n","#   if normalized == True:\n","#     p = 1\n","#     for i in x_shape:\n","#       p *= i\n","#     x /= math.sqrt(p)\n","#   assert signal_sizes == x.shape[1], \"こらっ！\"\n","#   return x\n","\n","\n","def fast_conv1d(signal, kernel):\n","  batch, ch, L_sig = signal.shape\n","  assert ch == 1\n","  kernel = kernel.reshape(1, -1)\n","  L_I = kernel.shape[1]\n","  L_F = 2 << (L_I - 1).bit_length()\n","  L_S = L_F - L_I + 1\n","\n","  device_ = signal.device\n","  pad_kernel = L_F - L_I\n","  FDir = torch.fft.rfft(torch.cat((kernel, torch.zeros(1, pad_kernel, device=device_)), dim=1))\n","\n","  signal_sizes = L_F\n","  len_pad = (L_S - L_sig % L_S) % L_S\n","  offsets = range(0, L_sig, L_S)\n","\n","  signal = torch.cat((signal, torch.zeros(batch, ch, len_pad, device=device_)), dim=2)\n","\n","  result = torch.zeros(batch, 1, offsets[-1] + L_F).to(device_)\n","  pad_slice = L_F - L_S\n","\n","  for idx_fr in offsets:\n","    idx_to_in = idx_fr + L_S\n","    idx_to_out = idx_fr + L_F\n","    to_rfft = torch.cat((signal[:, 0, idx_fr:idx_to_in],\n","                         torch.zeros(batch, pad_slice, device=device_)), dim=1)\n","\n","    to_mul = torch.fft.rfft(to_rfft, norm=\"ortho\")\n","    to_irfft = to_mul * FDir\n","\n","    conved_slice = torch.fft.irfft(to_irfft, norm=\"ortho\")\n","    result[:, 0, idx_fr: idx_to_out] += conved_slice\n","\n","  return result[:, :, :L_sig]"]},{"cell_type":"markdown","metadata":{"id":"d-53o6qtzEoB"},"source":["## モデルモジュール - ミキサー"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7xSuorUzJls"},"outputs":[],"source":["class Mixer(nn.Module):\n","  def forward(self, tracks, group_by=None):\n","    if group_by:\n","      return tracks[:, group_by, :].sum(dim=1)\n","    else:\n","      return tracks.sum(dim=1)"]},{"cell_type":"markdown","metadata":{"id":"ZqtAB28GzL9y"},"source":["## モデル訓練\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdEE8ge00Mev"},"outputs":[],"source":["class Trainer(object):\n","  def __init__(self, model):\n","    lr = learning_rate\n","    self.model = model\n","    self.loss_histories = {'training': None, 'test': None}\n","    n_bins = cqt_bins\n","    self.cqters = {\"c1\": PseudoCqt(SR_WAV, 64, 1 * 32.703195, n_bins, n_bins),\n","                   \"c2\": PseudoCqt(SR_WAV, 64, 2 * 32.703195, n_bins, n_bins),\n","                   \"c3\": PseudoCqt(SR_WAV, 64, 4 * 32.703195, n_bins, n_bins),\n","                   \"c4\": PseudoCqt(SR_WAV, 64, 8 * 32.703195, n_bins, n_bins),\n","                   \"c5\": PseudoCqt(SR_WAV, 64, 16 * 32.703195, n_bins, n_bins),\n","                   \"c6\": PseudoCqt(SR_WAV, 64, 32 * 32.703195, n_bins, n_bins),\n","                   \"c7\": PseudoCqt(SR_WAV, 64, 2000, n_bins, n_bins)}\n","                   # \"c8\": PseudoCqt(SR_WAV, 64, 4000, n_bins, n_bins)}\n","    self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n","    self.loss_domains = loss_domains\n","    metrics_dict = {'mae': F.l1_loss, 'mse': F.mse_loss}\n","    self.metric_funcs = [metrics_dict[m] for m in metrics]\n","    self.metric_names = metrics\n","    self.src_norm = source_norm\n","    self.hpss = compare_after_hpss\n","\n","    if 'melgram' in self.loss_domains:\n","      self.mel_n_fft = 1024\n","      self.mel_fb = nn.Parameter(torch.from_numpy(librosa.filters.mel(\n","          sr=SR, n_fft=self.mel_n_fft, n_mels=n_mels, fmin=0.0, fmax=SR / 2.0)).type(TCDTYPE))\n","    if 'stft' in self.loss_domains:\n","      self.n_fft = 1024\n","\n","    ddfs = [get_ddf_smt(), get_ddf_mdb()]\n","    self.evalers = [Evaluator(self.model, ddf, device=DEVICE) for ddf in ddfs]\n","    self.scores = {ddf.name: np.zeros((0, 4)) for ddf in ddfs}\n","\n","  def train(self, epoch, max_epoch, tr_loader):\n","    for i in range(epoch):\n","      cur_epoch = i + max_epoch + 1\n","      print()\n","      print(\"{}epoch目\".format(cur_epoch))\n","      self.train_epoch(tr_loader)\n","      torch.save(self.model.state_dict(), MODEL_PATH+f\"/{cur_epoch}epoch_model.pth\")\n","      # self.evaluate(result_subfolder=f\"{cur_epoch}epoch\")\n","\n","  def train_epoch(self, tr_loader):\n","    def _loss_a_batch(mixes):\n","      self.optimizer.zero_grad()\n","      mixes = mixes.to(DEVICE, non_blocking=True)\n","      mix, est_mix, est_irs = self.model(mixes)\n","\n","      losses = self._compute_loss(mix, est_mix, est_irs)\n","      loss = None\n","      for i, key in enumerate(losses):\n","        if i == 0:\n","          loss = losses[key]\n","        else:\n","          loss = loss + losses[key]\n","\n","      return losses, loss\n","\n","    def _train_a_batch(mixes):\n","      losses, loss = _loss_a_batch(mixes)\n","      loss.backward()\n","      self.optimizer.step()\n","      return losses\n","\n","    self.model.train()\n","    bar = tqdm(enumerate(tr_loader), total=len(tr_loader))\n","    accum_losses = defaultdict(lambda: 0.)\n","    for batch_i, mixes in bar:\n","      losses = _train_a_batch(mixes)\n","      desc = ' '.join('{}:{:4.2f}'.format(k, v) for (k, v) in losses.items())\n","      bar.set_description(desc)\n","      for key in losses:\n","        accum_losses[key] += dcnp(losses[key])\n","\n","\n","  def evaluate(self, max_epoch):\n","    with torch.no_grad():\n","      result_subfolder = f\"{max_epoch}epoch\"\n","\n","      self.model.eval()\n","      result_sub_path = os.path.join(RESULT_PATH, result_subfolder)\n","      os.makedirs(result_sub_path, exist_ok=True)\n","      keys = ['KD', 'SD', 'HH']\n","      for evaler in self.evalers:\n","        ddf_name = evaler.ddf.name\n","        print(ddf_name)\n","        path = os.path.join(result_sub_path, 'f1_scores_%s.pkl' % ddf_name)\n","\n","        evaler.predict(verbose=True)\n","        evaler.pickpeaks(pickpeak_fix, verbose=True)\n","        evaler.mir_eval()\n","        evaler.save_and_print_result(result_sub_path)\n","\n","        with open(path, 'wb') as f_write:\n","          pickle.dump(evaler.f_scores, f_write)\n","        np_path = os.path.join(result_sub_path, 'est_irs_%s.npz' % ddf_name)\n","        np.savez_compressed(np_path, *evaler.midis)\n","\n","  def _compute_loss(self, mixes, est_mixes, est_impulses):\n","    losses = defaultdict(lambda: 0.)\n","    if self.hpss:\n","      mixes = pss_src(mixes)\n","      est_mixes = pss_src(est_mixes)\n","\n","    if 'spectrum' in self.loss_domains:\n","      self._compute_spectrum_loss(mixes, est_mixes, losses=losses)\n","    if 'melgram' in self.loss_domains:\n","      self._compute_melgram_loss(mixes, est_mixes, losses=losses, weight=1.0)\n","    if 'stft' in self.loss_domains:\n","      self._compute_stft_loss(mixes, est_mixes, losses=losses, weight=1.0)\n","    if 'l1_reg' in self.loss_domains:\n","      losses['l1_reg'] = norm_losses(est_impulses, p=1, weight=self.args[\"l1_reg_lambda\"])\n","\n","    return losses\n","\n","  def _compute_melgram_loss(self, mix, est_mix, losses, weight=1.0):\n","    loss_melgram(mix, est_mix, self.mel_fb.to(mix.device),\n","                 self.mel_n_fft, self.metric_funcs,\n","                 self.metric_names, losses)\n","\n","  def _compute_stft_loss(self, mix, est_mix, losses, weight=1.0):\n","    loss_stft(mix, est_mix, self.n_fft, self.metric_funcs,\n","              self.metric_names, losses)\n","\n","  def _compute_spectrum_loss(self, mix, est_mix, losses, weight=1.0, perceptual=False):\n","    for cqter_key in self.cqters:\n","      cqter = self.cqters[cqter_key]\n","      est_cqt = cqter(est_mix)\n","      org_cqt = cqter(mix)\n","\n","      for metric_name, metric in zip(self.metric_names, self.metric_funcs):\n","        loss = weight * 4 * metric(org_cqt, est_cqt)\n","        if not torch.isnan(loss):\n","          losses[cqter_key + metric_name] = loss\n","        else:\n","          print('%s was NaN, it is not added to the total loss' % cqter_key)\n","\n","def dcnp(torch_array):\n","  return torch_array.detach().cpu().numpy()"]},{"cell_type":"markdown","metadata":{"id":"xtpxDOLMwSaq"},"source":["## 訓練モジュール - CQT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w39wt4vGwXwL"},"outputs":[],"source":["cqt_filter_fft = librosa.constantq.__cqt_filter_fft\n","\n","class PseudoCqt:\n","  def __init__(self, sr=22050, hop_length=512, fmin=None, n_bins=84, bins_per_octave=12,\n","               tuning=0.0, filter_scale=1, norm=1, sparsity=0.01, window='hann', scale=True,\n","               pad_mode='reflect'):\n","\n","        fft_basis, n_fft, _ = cqt_filter_fft(sr, fmin, n_bins, bins_per_octave,\n","                                             filter_scale, norm, sparsity,\n","                                             hop_length=hop_length, window=window)\n","\n","        self.fft_basis = torch.tensor(np.array(np.abs(fft_basis.todense())), dtype=TCDTYPE,\n","                                      device=DEVICE)\n","\n","        self.fmin = fmin\n","        self.fmax = fmin * 2 ** (float(n_bins) / bins_per_octave)\n","        self.n_fft = n_fft\n","        self.hop_length = hop_length\n","        self.pad_mode = pad_mode\n","        self.scale = scale\n","        win = torch.zeros((self.n_fft,), device=DEVICE)\n","        win[self.n_fft // 2 - self.n_fft // 8:self.n_fft // 2 + self.n_fft // 8] = torch.hann_window(self.n_fft // 4)\n","        self.window = win\n","\n","  def __call__(self, y):\n","        return self.forward(y)\n","\n","  def forward(self, y):\n","        mag_stfts = torch.stft(y, self.n_fft,\n","                               hop_length=self.hop_length,\n","                               window=self.window, return_complex=False).pow(2).sum(-1)\n","        mag_stfts = torch.sqrt(mag_stfts + EPS)\n","        # C_torch = torch.stack([torch.sparse.mm(self.fft_basis, D_torch_row) for D_torch_row in D_torch])\n","        mag_melgrams = torch.matmul(self.fft_basis, mag_stfts)\n","\n","        mag_melgrams /= torch.tensor(np.sqrt(self.n_fft), device=y.device)\n","        return to_log(mag_melgrams)\n","\n","def to_log(mag_specgrams):\n","  return (torch.log10(mag_specgrams+EPS)-torch.log10(torch.tensor(EPS, device=mag_specgrams.device)))"]},{"cell_type":"markdown","metadata":{"id":"ghhNgjiRA30h"},"source":["## 評価データセットの取得"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwoxMHboA6zQ"},"outputs":[],"source":["# IDMT-SMT-DRUMSの取得\n","def get_ddf_smt():\n","  ddf_smt = TestData(os.path.join(EVALDATA_PATH, 'SMT_DRUMS'), 'smt',\n","                              label_map=None, ann_folder='annotations',\n","                              audio_folder='audio')\n","  return ddf_smt\n","\n","\n","# ENST-DRUMSの取得\n","def get_ddf_enst():\n","  ddf_enst = TestData(os.path.join(EVALDATA_PATH, 'ENST_DTP(wet_mix-minus_one)'), 'enst',\n","                      label_map={'0': 'KD', '1': 'SD', '2': 'HH'},\n","                      ann_folder='annotations', audio_folder='audio')\n","  return ddf_enst\n","\n","# MDBの取得\n","def get_ddf_mdb():\n","  ddf_mdb = TestData(os.path.join(EVALDATA_PATH, 'MDB_Drums'), 'mdb',\n","                              label_map=None, ann_folder='annotations/class',\n","                              audio_folder='audio/drum_only')\n","  return ddf_mdb\n","\n","\n","# 評価データの格納\n","class TestData:\n","  def __init__(self, path, name, label_map=None, ann_folder='annotations', audio_folder='audio'):\n","    self.name = name\n","    self.path = path\n","    self.label_map = label_map\n","    self.anno_fns, self.audio_fns = [], []\n","    self.ann_folder = ann_folder\n","    self.audio_folder = audio_folder\n","    self._scan_files()\n","\n","  def _scan_files(self):\n","    anno_fns = os.listdir(os.path.join(self.path, self.ann_folder))\n","    anno_fns = [f for f in anno_fns if f.endswith('.txt')]\n","    self.anno_fns = sorted(anno_fns)\n","\n","    audio_fns = os.listdir(os.path.join(self.path, self.audio_folder))\n","    audio_fns = [f for f in audio_fns if f.endswith('.wav')]\n","    self.audio_fns = sorted(audio_fns)\n","    assert len(self.anno_fns) == len(self.audio_fns), 'The number of files should be equal but %d and %d' % (len(self.anno_fns), len(self.audio_fns))\n","    self.n_files = len(self.audio_fns)\n","\n","  def __iter__(self):\n","    self.n = 0\n","    return self\n","\n","  def __len__(self):\n","    return self.n_files\n","\n","  def __next__(self):\n","    if self.n < self.n_files:\n","      anno_fn = self.anno_fns[self.n]\n","      audio_fn = self.audio_fns[self.n]\n","      src, _ = librosa.load(os.path.join(self.path, self.audio_folder, audio_fn), sr=SR)\n","      onsets_tuple = mir_eval.io.load_labeled_events(os.path.join(self.path, self.ann_folder, anno_fn))\n","      onsets_dict = read_annotations_multilabel(onsets_tuple)\n","      onsets_dict = rename_key(onsets_dict, self.label_map)\n","      self.n += 1\n","      return src, onsets_dict\n","    else:\n","      raise StopIteration\n","\n","\n","warnings.filterwarnings(action='once')\n","\n","\n","def process_annotation(txtpath_r, txtpath_w, label_map, delimiter='\\t'):\n","    \"\"\"\n","    Args:\n","        txtpath_r (str): path to load the annotation\n","        txtpath_w (str): path to write\n","        label_map (dict): a dict, like {0:'KD', 1:'SD'} to process, for example\n","        delimiter (str): delimiter between events\n","    \"\"\"\n","    with open(txtpath_r, 'r') as f_r:\n","        with open(txtpath_w, 'w') as f_w:\n","            for line in f_r:\n","\n","                t, old_label = line.rstrip('\\n').split(delimiter)\n","                t, old_label = t.strip(), old_label.strip()\n","                if label_map is not None:\n","                    new_label = label_map[old_label]\n","                else:\n","                    new_label = old_label\n","                f_w.write(delimiter.join([t, new_label]))\n","                f_w.write('\\n')\n","\n","\n","def read_annotations_multilabel(onsets_tuple):\n","  labels = set(onsets_tuple[1])\n","  onsets_dict = {k: [] for k in labels}\n","  for t, label in [list(i) for i in zip(*onsets_tuple)]:\n","    onsets_dict[label].append(t)\n","\n","  for key in onsets_dict:\n","    onsets_dict[key] = np.array(onsets_dict[key])\n","  return onsets_dict\n","\n","\n","def rename_key(old_dict, key_map=None):\n","  if key_map is None:\n","    return old_dict\n","  for old_key in old_dict:\n","    if old_key in key_map and old_key != key_map[old_key]:\n","      new_key = key_map[old_key]\n","      old_dict[new_key] = old_dict[old_key]\n","      del old_dict[old_key]\n","  return old_dict\n","\n","\n","def pickpeak_fix(impulse):\n","  div_max, div_avg, div_wait, div_thre = 20, 10, 16, 4\n","\n","  impulse /= impulse.max()\n","  peak_idxs = librosa.util.peak_pick(impulse, SR // div_max, SR // div_max, SR // div_avg,\n","                                     SR // div_avg, 1.0 / div_thre, SR // div_wait)\n","\n","  return librosa.samples_to_time(peak_idxs, sr=SR)"]},{"cell_type":"markdown","metadata":{"id":"jaQqv_h3_hf9"},"source":["## 評価"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9k8UBSHQ_f9Q"},"outputs":[],"source":["class Evaluator(object):\n","  def __init__(self, model, ddf, device='cpu'):\n","    self.device = device\n","    self.model = model.to(self.device)\n","    self.ddf = ddf\n","    self.component_names = ['KD', 'SD', 'HH']\n","        \n","    self.lst = 1024\n","    self.max_nsp = self.lst * 100\n","    self.reset_data()\n","    self.midis = [None] * len(self.ddf)\n","    self.est_onsets = [None] * len(self.ddf)\n","    self.ref_onsets = [None] * len(self.ddf)\n","    self.reset_data()\n","\n","  def reset_data(self):\n","    self.ndc = self.n_drum_components = len(self.component_names)\n","    self.f_scores = {k: [] for k in self.component_names}\n","\n","  def save_and_print_result(self, path):\n","    if self.f_scores != {}:\n","      print('Means of F/P/R, Stds of F/P/R')\n","      for key in self.f_scores:\n","        songs_score = np.array(self.f_scores[key])\n","        mean_score, std_score = np.mean(songs_score, axis=0), np.std(songs_score, axis=0)\n","        print(key, mean_score, std_score)\n","        np.save(os.path.join(path, f\"{self.ddf.name}_mean_score_{key}.npy\"), arr=mean_score)\n","        np.save(os.path.join(path, f\"{self.ddf.name}_std_score_{key}.npy\"), arr=std_score)\n","    else:\n","      print('self.f_scores is blank, so nothing to print.')\n","\n","  def predict(self, verbose=False):\n","    def send_pred_reduce(src):\n","      # pad = 4080\n","      # src = np.concatenate([np.zeros(pad, ), src, np.zeros(pad, )], axis=0)\n","      src = torch.tensor(src[np.newaxis, :], dtype=TCDTYPE).to(self.device)\n","      ret = self.model.forward(src)\n","      est_irs = ret[2]\n","      est_irs = est_irs[0].detach().cpu().numpy()\n","      return np.stack([est_irs[0], est_irs[1], est_irs[2:5].sum(axis=0)], axis=0).astype(np.float32)\n","\n","    ddf_iter = iter(self.ddf)\n","    if verbose:\n","      bar = tqdm(enumerate(ddf_iter), total=len(self.ddf), desc='predicting..')\n","    else:\n","      bar = enumerate(ddf_iter)\n","\n","    for song_idx, (src, onsets_dict) in bar:\n","      # prepare - make it multiple of lst\n","      len_pad = (self.lst - len(src) % self.lst) % self.lst\n","      if len_pad != 0:\n","        src = np.concatenate((src, np.zeros(len_pad, )), axis=0)\n","      src = src.astype(NPDTYPE)\n","      src = src / np.abs(src).max()\n","\n","      # Do the prediction\n","      if len(src) >= self.max_nsp:\n","        has_residual = (len(src) % self.max_nsp) != 0\n","        midis = np.zeros((self.ndc, 0), dtype=np.float32)\n","        for i in range(len(src) // self.max_nsp + int(has_residual)):\n","          sub_midis = send_pred_reduce(src[i * self.max_nsp: (i + 1) * self.max_nsp])\n","          midis = np.concatenate((midis, sub_midis), axis=1)\n","      else:\n","        midis = send_pred_reduce(src)\n","      self.midis[song_idx] = midis.astype(np.float32)\n","\n","  def pickpeaks(self, pp_func, verbose=False, **kwargs):\n","    ddf_iter = iter(self.ddf)\n","    if verbose:\n","      bar = tqdm(enumerate(ddf_iter), total=len(ddf_iter), desc='picking peaks...')\n","    else:\n","      bar = enumerate(ddf_iter)\n","\n","    for song_idx, (src, onsets_dict) in bar:\n","      est_onset_song = []\n","      ref_onset_song = []\n","      for i, key in zip(range(self.ndc), self.component_names):\n","        est_onset = pp_func(self.midis[song_idx][i])  # onset positions\n","        est_onset_song.append(est_onset)\n","\n","        if key in onsets_dict:\n","          ref_onset = onsets_dict[key]\n","        else:\n","          ref_onset = np.array([])\n","        ref_onset_song.append(ref_onset)\n","      self.est_onsets[song_idx] = np.array(est_onset_song, dtype=object)\n","      self.ref_onsets[song_idx] = np.array(ref_onset_song, dtype=object)\n","\n","  def mir_eval(self):\n","    self.reset_data()\n","    for ref_onset, est_onset in zip(self.ref_onsets, self.est_onsets):\n","      for i, key in enumerate(self.component_names):\n","        f_score = mir_eval.onset.f_measure(ref_onset[i], est_onset[i])  # F, P, R\n","        self.f_scores[key].append(f_score)\n","\n","  def illustrate_one(self, song_idx, img_folder, verbose=False):\n","    midis = self.midis[song_idx]\n","    est_onset = self.est_onsets[song_idx]\n","    ref_onset = self.ref_onsets[song_idx]\n","    if midis is None:\n","      if verbose:\n","        print('none...')\n","      return None\n","\n","    # FIGURE 1\n","    plt.figure(figsize=(15, 3))\n","    for i in range(3):\n","      plt.subplot(3, 3, i + 1)\n","      display.waveplot(midis[i])\n","      plt.title(self.component_names[i] + ' est_irs')\n","      if i == 0:\n","        plt.title(self.component_names[i] + ' est_irs ' + str(song_idx) + ' ' + self.ddf.audio_fns[song_idx])\n","\n","    for i, key in zip(range(self.ndc), self.component_names):  # KD, SD, HH\n","      # FIGURE 2\n","      plt.subplot(3, 3, i + 4)\n","      tmp = np.zeros_like(midis[i])\n","      np.put(tmp, librosa.time_to_samples(est_onset[i], sr=SR), np.ones(len(est_onset[i])))\n","      display.waveplot(tmp)\n","      plt.title('after peak picking')\n","      # FIGURE 3\n","      plt.subplot(3, 3, i + 7)\n","      tmp = np.zeros_like(midis[i])\n","      np.put(tmp, librosa.time_to_samples(ref_onset[i], sr=SR), np.ones(len(ref_onset[i])))\n","      display.waveplot(tmp)\n","      plt.title('reference')\n","      plt.savefig(os.path.join(img_folder + '/' + self.ddf.anno_fns[song_idx] + '.png'))\n","      if verbose:\n","        print('-%s: %3.0d %3.0d' % (key, len(ref_onset[i]), len(est_onset[i])), end='   ')\n","    if verbose:\n","      print('')\n","\n","  def illustrate(self, img_folder):\n","    bar = tqdm(range(len(self.ddf)), total=len(self.ddf), desc='drawing..')\n","    for song_idx in bar:\n","      self.illustrate_one(song_idx, img_folder)"]},{"cell_type":"markdown","metadata":{"id":"LB8UEDTk0NCu"},"source":["## 学習データセット"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwEBiHMBHh6j"},"outputs":[],"source":["class DrumDataset(Dataset):\n","  def __init__(self, dir1, dir2):\n","    super().__init__()\n","    dir1 = dir1\n","    dir2 = dir2\n","    self.files = glob.glob(dir1+\"/*\")\n","    self.files.extend(glob.glob(dir2+\"/*\"))\n","    \n","  def __getitem__(self, idx):\n","    path = self.files[idx]\n","    src, _  = librosa.load(path, sr=SR_WAV, mono=True, duration=DURATION)\n","    return torch.from_numpy(librosa.util.normalize(src, axis=0))\n","\n","  def __len__(self):\n","    return len(self.files)\n","\n","\n","def load_audio_file(filename, sample_rate=None, num_channels=None,\n","                    channel=None, start=None, stop=None, dtype=None,\n","                    replayagain_mode=None, replayagain_preamp=0.0):\n","    signal, file_sample_rate = librosa.core.load(filename, sr=sample_rate, mono=True)\n","    if start is not None:\n","        start = int(start * file_sample_rate)\n","    if stop is not None:\n","        stop = min(len(signal), int(stop * file_sample_rate))\n","    if start is not None or stop is not None:\n","        signal = signal[start: stop]\n","    return signal, file_sample_rate\n","\n","\n","class TxtSrcDataset(Dataset):\n","    def __init__(self, txt_path, src_path, duration, sr_wav, ext='mp3'):\n","        super(TxtSrcDataset, self).__init__()\n","        self.txt_path = txt_path\n","        self.src_path = src_path\n","        self.duration = duration\n","        self.sr_wav = sr_wav\n","        self.ext = ext\n","        self.lines = []\n","        self._read_txt()\n","\n","    def _read_error(self, size):\n","        raise NotImplementedError()\n","\n","    def _read_txt(self):\n","        raise NotImplementedError()\n","\n","    def _read_audio(self, path, duration, file_dura):\n","        raise NotImplementedError()\n","\n","    def _line_to_readpath(self, idx):\n","        raise NotImplementedError()\n","\n","    def __len__(self):\n","        return len(self.lines)\n","\n","    def __getitem__(self, idx):\n","        path, file_dura = self._line_to_readpath(idx)\n","        mix = self._read_audio(path, duration=self.duration, file_dura=file_dura)\n","        return torch.from_numpy(mix), torch.zeros(mix.shape), torch.zeros(mix.shape)\n","\n","\n","class TxtDrumstemDataset(TxtSrcDataset):\n","    \"\"\"textfile-based datast but for drum stems\n","    \"\"\"\n","    def __init__(self, *args, **kwargs):\n","        super(TxtDrumstemDataset, self).__init__(*args, **kwargs)\n","\n","    def _read_error(self, size):\n","        return np.random.uniform(-0.01, 0.01, size=size).astype(NPDTYPE)\n","\n","    def _read_txt(self):\n","\n","        with open(self.txt_path) as f_read:\n","            for idx, line in enumerate(f_read):\n","                if int(float(line.rstrip('\\n').split('\\t')[1])) > self.duration + 1:\n","                    self.lines.append(line.rstrip('\\n'))\n","\n","    def _read_audio(self, path, duration, file_dura):\n","        start = np.random.choice(int(file_dura - duration))  # [second]\n","        try:\n","            src, _ = load_audio_file(path, sample_rate=SR_WAV, dtype=NPDTYPE, num_channels=1,\n","                                                     start=start, stop=start + duration)\n","\n","            if len(src) < int(SR_WAV * duration):\n","                return np.concatenate(\n","                    (src, np.random.uniform(-0.01, 0.01, (NSP_SRC - len(src))).astype(NPDTYPE)), axis=0)\n","            return librosa.util.normalize(src, axis=0)\n","\n","        except Exception as e:\n","            sys.stderr.write('AUDIO READ ERROR (%s): %s\\n' % (path, e))\n","            return self._read_error(size=(int(SR_WAV * duration),))\n","\n","    def _line_to_readpath(self, idx):\n","        filename, file_duration = self.lines[idx].split('\\t')\n","        return '%s/%s' % (self.src_path, filename), float(file_duration)"]},{"cell_type":"markdown","metadata":{"id":"6VMxiinVmCjc"},"source":["# main(train・eval同)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Alvtsu3Cv7Vu"},"outputs":[],"source":["warnings.filterwarnings('ignore', module='matplotlib')\n","\n","#torch.multiprocessing.set_start_method('spawn', force=True)\n","\n","torch.backends.cudnn.benchmark = True\n","\n","inst_srcs = load_drum_srcs(idx=N_DRUM_VSTS)\n","inst_names = DRUM_NAMES\n","\n","drum_sets = get_drumsets(source_norm)\n","model = DrumTranModel(inst_srcs, inst_names, drum_sets)\n","saved_models = os.listdir(MODEL_PATH)\n","max_epoch = 0\n","if saved_models:\n","  model_epochs = [int(model.split(\"epoch\")[0]) for model in saved_models]\n","  max_epoch = max(model_epochs)\n","  model.load_state_dict(torch.load(MODEL_PATH+\"/\"+str(max_epoch)+\"epoch_model.pth\"))\n","  print(f\"load {max_epoch}epoch_model\")\n","model = model.to(DEVICE)\n","trainer = Trainer(model)\n","\n","epoch = 5\n","start_num = 1\n","\n","for i in range(10):\n","  drumstem_dataset = DrumDataset(STEM_PATH1+str(i+start_num), STEM_PATH2+str(i+start_num))\n","  tr_params = {\"batch_size\": batch_size, \"shuffle\": True, \"num_workers\": 2, \"pin_memory\":True, \"drop_last\": True}\n","  train_loader = DataLoader(drumstem_dataset, **tr_params)\n","  trainer.train(epoch, max_epoch, train_loader)\n","  trainer.evaluate(max_epoch)\n","  max_epoch += epoch"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["p71G9lgOjTOV","x-KHoHd9MFCj","sQwOb86FjiJJ","zJoKYrjgkBqI","UTXTBVj3bK0a","zcCsnkioD1qG","toU9W9I04ghm","4MKGHSy4AIb9","riOJUf-ktbs0","dZyDzppVuDxr","zmRjpEQtuLuw","3OnThtr-uR5J","d-53o6qtzEoB","ZqtAB28GzL9y","xtpxDOLMwSaq","ghhNgjiRA30h","jaQqv_h3_hf9","LB8UEDTk0NCu","6VMxiinVmCjc"],"name":"Drummernet_remake.ipynb","provenance":[],"mount_file_id":"1eE3QawhZzZXLkL_3Hio4VliZAMRlna-F","authorship_tag":"ABX9TyPYxnDzAvmh38fE6THEw0Fc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}